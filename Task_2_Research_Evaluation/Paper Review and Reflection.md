# Paper Review

The paper "ImageNet Classification with Deep Convolutional Neural Networks" by Krizhevsky, Sutskever, and Hinton proposes a deep CNN architecture now widely known as AlexNet that quite significantly raises the bar for large-scale image classification. The authors tackle the challenging task of object recognition for 1.2 million high-resolution images across the 1000 classes of the ImageNet dataset. Their architecture involves five convolutional layers followed by three fully connected layers, with ReLU activations to speed up training and dropout for reducing overfitting. Moreover, they perform data augmentation and GPU parallelization for efficiently handling this huge dataset. On ILSVRC-2010, this network results in a top-1 error of 37.5% and a top-5 error of 17.0%, outperforming traditional hand-designed feature methods by a large margin. Their results confirm that deep learning, provided with data and computation, can outperform classical computer-vision techniques by learning features hierarchically from raw pixels.

# Reflection

This felt like reading a turning point in AI history. It is strange to think about how deep learning is basically everywhere, but this must have been quite bold at the time; nonetheless, feeding raw pixel data to that enormous neural network instead of hand-crafted features was really a paradigm shift. The most interesting part to me was just how much many of the things that are basically considered "standard" nowadays- ReLU, dropout, data augmentation, GPU training-all were once novel and necessary inventions just to get this system to work.

This is a limitation in that it relies on enormous labeled datasets and computationally intensive processing. Whereas it still took 5â€“6 days to train the network on two GPUs, the model used 60 million parameters, which is not trivial. I wonder how this approach would have been accessible to smaller research groups at that time. A possible extension of this work could be an investigation of more efficient architectures or unsupervised pretraining, as hinted at by the authors. Looking from today's perspective, modern techniques such as transfer learning, batch normalization, or deeper architectures like ResNet demonstrate how the field evolved to address efficiency and accuracy together. Overall, the paper feels foundational-it is as if one watched the moment when deep learning stepped into the computer vision spotlight.
